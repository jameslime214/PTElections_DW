import os
import pandas as pd
import psycopg2


def run_sql_in_server(file_path: str, _cursor: psycopg2.extensions.cursor):
    """
    Run a SQL file in the PostgreSQL server using the provided cursor.
    """
    ## Ensure filetype is .sql
    if os.path.splitext(file_path)[1].lower() != '.sql':
        raise ValueError("Unsupported file format. This function only processes .sql files.")
    else:
        print(f"Running SQL file: {file_path}")
        with open(file_path, 'r') as file:
            _cursor.execute(file.read())
    

def read_data_from_table(table_name: str, _cursor: psycopg2.extensions.cursor):
    """
    Retrieve and print all records from the specified database table.

    This function executes a SQL query to select every record from the table 
    identified by 'table_name'. It then fetches the resulting rows and prints each 
    one to the standard output.

    Parameters:
        table_name: The name of the table from which to retrieve data.
        _cursor: A database cursor object used to execute SQL commands.

    Returns:
        None

    Note:
        Ensure that 'table_name' is a trusted input, as it is directly inserted into the SQL statement.
        For untrusted input, consider using parameterized queries to prevent SQL injection.
    """
    _cursor.execute(f"SELECT * FROM {table_name};")
    rows = _cursor.fetchall()
    for row in rows:
        print(row)


def iter_filepaths(directory: str):
    """
    Yields the full path for each file in the specified directory.
    
    Parameters:
        directory: The path to the directory.
        
    Yields:
        str: The full file path of each file in the directory.
    """
    for filename in os.listdir(directory):
        filepath = os.path.join(directory, filename)
        if os.path.isfile(filepath):
            yield filepath
    

def extract_dataframe_from_txt(file_path: str, *, _encoding: str = 'utf-8') -> pd.DataFrame:
    """
    Extract data from a text file into a pandas DataFrame without any cleaning.
    
    Parameters:
        file_path: Path to the input text file
        encoding: Character encoding to use when reading the file
        
    Returns:
        pandas.DataFrame: Raw data extracted from the text file
        
    Raises:
        ValueError: If the file is not a .txt file or if an error occurs during reading
    """
    ## Ensure the file has a .txt extension
    if os.path.splitext(file_path)[1].lower() != '.txt':
        raise ValueError("Unsupported file format. This function only processes .txt files.")

    try:
        ## Read the file as fixed-width format without header
        df = pd.read_fwf(file_path, encoding= _encoding, header= None)
        return df
    except Exception as e:
        raise ValueError(f"Error reading fixed-width file: {str(e)}")


def extract_dataframe_from_excel(file_path: str) -> pd.DataFrame:
    """
    Extract data from an Excel file into a pandas DataFrame.
    
    Parameters:
        file_path: Path to the input Excel file (.xls or .xlsx)
        
    Returns:
        pandas.DataFrame: Data extracted from the Excel file
        
    Raises:
        ValueError: If the file is not an Excel file or if an error occurs during reading
    """
    ## Ensure the file has an Excel extension
    ext = os.path.splitext(file_path)[1].lower()
    if ext not in ['.xls', '.xlsx']:
        raise ValueError("Unsupported file format. This function only processes Excel files.")

    try:
        ## Read the Excel file
        df = pd.read_excel(file_path)
        return df
    except Exception as e:
        raise ValueError(f"Error reading Excel file: {str(e)}")


def save_dataframe_to_excel(df: pd.DataFrame, output_path: str) -> None:
    """
    Save a DataFrame to an Excel file without overwriting an existing file.
    
    If an Excel file already exists at the provided output_path, a new file name
    will be generated by appending an underscore and a counter to the base file name.
    
    Parameters:
        df: DataFrame to save
        output_path: Full path where the Excel file will be saved
        
    Returns:
        None
    """
    ## Ensure the directory exists
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    ## Split the file name into base and extension
    base, ext = os.path.splitext(output_path)
    counter = 1
    
    ## Loop until we find a file name that doesn't exist
    while os.path.exists(output_path):
        output_path = f"{base}_{counter}{ext}"
        counter += 1
    
    ## Write the DataFrame to the new Excel file
    try:
        with pd.ExcelWriter(output_path, engine='openpyxl', mode='w') as writer:
            df.to_excel(writer, index=False)
        saved_filename = os.path.basename(output_path)
        target_directory = os.path.dirname(output_path)
        print(f"Successfully saved {saved_filename} to {target_directory}")
    except Exception as e:
        raise ValueError(f"Error writing Excel file: {str(e)}")


def parse_FC_data_1(df: pd.DataFrame) -> pd.DataFrame:
    """
    Remove columns from a DataFrame where all values are either 0, '0', or null.
    
    This function is intended for cleaning the dataframes extracted from text files
    where some columns contain only zeros or null values and don't contribute 
    meaningful information.
    
    Parameters:
        df: Input DataFrame to clean.
        
    Returns:
        pandas.DataFrame: DataFrame with zero-only columns removed.
    """
    ## Create a copy to avoid modifying the original DataFrame
    cleaned_df = df.copy()
    
    ## List to store columns that should be kept
    columns_to_keep = []
    
    for col in cleaned_df.columns:
        ## Check if column contains only 0s (numeric), '0' (string), or null values
        is_all_zeros = cleaned_df[col].apply(
            lambda x: x == 0 or x == '0' or pd.isna(x)
        ).all()
        
        ## If not all zeros, keep the column
        if not is_all_zeros:
            columns_to_keep.append(col)
    
    ## Return DataFrame with only the columns to keep
    return cleaned_df[columns_to_keep]


def parse_FC_data_2(df: pd.DataFrame) -> pd.DataFrame:
    """
    Process a DataFrame containing municipal or parish data where the first column contains 
    combined numeric + text data (e.g. "123456Municipality Name").
    
    This function splits the first column into two parts:
      - A new leftmost column containing the numeric code.
      - The original first column is replaced with only the non-numeric (text) part.
    
    Parameters:
        df: Input DataFrame.
        
    Returns:
        pandas.DataFrame: Modified DataFrame with split first column.
    """
    ## Create a copy to avoid modifying the original DataFrame
    new_df = df.copy()
    
    ## Ensure values are treated as strings
    col0 = new_df[0].astype(str)

    ## Extract numeric part from the first column using a regex
    numeric_part = col0.str.extract(r'^(\d+)')[0].astype('int64')
    
    ## Extract text part from the first column (trim any whitespace)
    text_part = col0.str.extract(r'^\d+(.*)$')[0].str.strip()
    
    ## Create a new DataFrame with the extracted parts and all remaining columns
    result_df = pd.concat([numeric_part, text_part, new_df.iloc[:, 1:]], axis=1)
    
    return result_df


def parse_FC_data_3(df: pd.DataFrame, file_name: str) -> pd.DataFrame:
    """
    Process the columns between location name and voting data in FC (freguesia/conselho) dataframes.
    
    Different files have different formats:
    - ar76c/f: Contains one column between name and total votes info
    - ar79c/f and later: Contains three columns between name and total votes info
    
    This function handles these differences and ensures the intermediate columns are properly formatted as numerical data.
    
    Parameters:
        df: Input DataFrame (after applying parse_FC_data_1 and parse_FC_data_2)
        file_name: Base name of the file (e.g., 'Ar76c', 'AR79F') to determine format
        
    Returns:
        pandas.DataFrame: DataFrame with intermediate columns properly formatted
    """
    ## Create a copy to avoid modifying the original DataFrame
    result_df = df.copy()
    
    ## Lowercase the file name for consistent comparisons
    file_name_lower = file_name.lower()
    
    ## The first two columns are now: code and name (after parse_FC_data_2)
    ## We need to identify where the 4 total votes columns start
    
    ## Find the index of the first column of party info (which is an object/string type)
    ## After the total votes columns (which should be 4 columns with numeric data)
    party_cols_start = None
    
    ## Based on notes, the starting column for party information varies by file
    if 'ar76' in file_name_lower:
        ## For ar76 files, party columns start at index 6
        party_cols_start = 6
    else:
        ## For ar79 and later files, party columns start at index 8
        party_cols_start = 8
    
    ## The 4 total votes columns start 4 positions before the party columns
    total_votes_start = party_cols_start - 4
    
    ## Columns between name column (index 1) and total_votes_start need processing
    if 'ar76' in file_name_lower:
        ## For ar76 files, there's one column to process
        if len(result_df.columns) > 2:  # Ensure we have at least one column to process
            ## Convert the intermediate column to numeric, with errors coerced to NaN
            result_df[2] = pd.to_numeric(result_df[2], errors='coerce')
            
            ## If the value is 500, it means no voting data for this location
            ## Other values represent actual data
            ## Values should already be numeric after parse_FC_data_1 removed '0' columns
    else:
        ## For ar79 and later files, there are three columns to process (or fewer if some were removed by parse_FC_data_1)
        ## The second column (index 3) contains useful data, the others were likely removed by parse_FC_data_1
        column_indices = [i for i in range(2, total_votes_start)]
        for idx in column_indices:
            if idx in result_df.columns:
                result_df[idx] = pd.to_numeric(result_df[idx], errors='coerce')
    
    return result_df


def generate_ddl_from_file(file_path: str) -> tuple[pd.DataFrame, str]:
    """
    Generate SQL Data Definition Language (DDL) statement and insertion commands from a CSV or Excel file.

    This function reads the provided file (CSV, XLS, or XLSX) into a pandas DataFrame, maps its data types to SQL
    types, and constructs a CREATE TABLE statement along with INSERT commands for each row of data. This facilitates
    an ETL process by generating both the table definition and the corresponding data insertion queries.

    Parameters:
        file_path (str): Path to the input file (.csv, .xls, or .xlsx).

    Returns:
        tuple: A tuple containing:
            - pandas.DataFrame: The data read from the input file.
            - str: The SQL DDL statement including the table creation and insertion commands.

    Raises:
        ValueError: If the file format is unsupported or if an error occurs during file reading.
    """
    ## Extract the file extension and convert it to lowercase (remove the leading dot)
    _, ext = os.path.splitext(file_path)
    file_extension = ext.lower()[1:]

    ## Read the file into a DataFrame based on its extension
    if file_extension == 'csv':
        df = pd.read_csv(file_path)
    elif file_extension in ['xls', 'xlsx']:
        df = pd.read_excel(file_path)
    else:
        raise ValueError("Unsupported file format. Please use .csv, .xls, or .xlsx files")

    ## Define mapping from pandas data types to SQL data types
    dtype_mapping = {
        'object': 'TEXT',
        'int64': 'INTEGER',
        'float64': 'NUMERIC',
        'datetime64[ns]': 'TIMESTAMP',
        'bool': 'BOOLEAN'
    }
    
    ## Extract the table name from the file path (use the file name without extension, in lowercase)
    table_name = os.path.splitext(os.path.basename(file_path))[0].lower()
    
    ## Build a list of column definitions for the CREATE TABLE statement
    columns = []
    for column, dtype in df.dtypes.items():
        sql_type = dtype_mapping.get(str(dtype), 'TEXT')
        columns.append(f'    "{column}" {sql_type}')
    
    ## Construct the CREATE TABLE DDL statement
    ddl = f"CREATE TABLE IF NOT EXISTS {table_name} (\n"
    ddl += ",\n".join(columns)
    ddl += "\n);"
    
    ## Prepare a comma-separated list of column names for the INSERT statements
    columns_list = ", ".join([f'"{col}"' for col in df.columns])
    insert_statements = []
    
    ## Generate an INSERT statement for each row in the DataFrame
    for index, row in df.iterrows():
        values = []
        for col in df.columns:
            if pd.isna(row[col]):
                values.append('NULL')
            elif isinstance(row[col], (int, float)):
                values.append(str(row[col]))
            elif isinstance(row[col], bool):
                values.append('TRUE' if row[col] else 'FALSE')
            elif isinstance(row[col], str):
                ## Escape single quotes in string values by replacing them with two single quotes
                values.append(f"'{row[col].replace('\'', '\'\'')}'")
            else:
                values.append(str(row[col]))
        values_str = ", ".join(values)
        insert_statements.append(f"INSERT INTO {table_name} ({columns_list}) VALUES ({values_str});")
    
    ## Append the INSERT statements to the DDL with a separating newline
    ddl += "\n\n" + "\n".join(insert_statements)
    ddl += "\n"
    
    return df, ddl
    