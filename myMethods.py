import os
import pandas as pd
import psycopg2


def run_sql_in_server(file_path: str, _cursor: psycopg2.extensions.cursor):
    """
    Run a SQL file in the PostgreSQL server using the provided cursor.
    """
    ## Ensure filetype is .sql
    if os.path.splitext(file_path)[1].lower() != '.sql':
        raise ValueError("Unsupported file format. This function only processes .sql files.")
    else:
        print(f"Running SQL file: {file_path}")
        with open(file_path, 'r') as file:
            _cursor.execute(file.read())
    

def read_data_from_table(table_name: str, _cursor: psycopg2.extensions.cursor):
    """
    Retrieve and print all records from the specified database table.

    This function executes a SQL query to select every record from the table 
    identified by 'table_name'. It then fetches the resulting rows and prints each 
    one to the standard output.

    Parameters:
        table_name: The name of the table from which to retrieve data.
        _cursor: A database cursor object used to execute SQL commands.

    Returns:
        None

    Note:
        Ensure that 'table_name' is a trusted input, as it is directly inserted into the SQL statement.
        For untrusted input, consider using parameterized queries to prevent SQL injection.
    """
    _cursor.execute(f"SELECT * FROM {table_name};")
    rows = _cursor.fetchall()
    for row in rows:
        print(row)


def iter_filepaths(directory: str):
    """
    Yields the full path for each file in the specified directory.
    
    Parameters:
        directory: The path to the directory.
        
    Yields:
        str: The full file path of each file in the directory.
    """
    for filename in os.listdir(directory):
        filepath = os.path.join(directory, filename)
        if os.path.isfile(filepath):
            yield filepath
    

def extract_dataframe_from_txt(file_path: str, *, _encoding: str = 'utf-8') -> pd.DataFrame:
    """
    Extract data from a text file into a pandas DataFrame without any cleaning.
    
    Parameters:
        file_path: Path to the input text file
        encoding: Character encoding to use when reading the file
        
    Returns:
        pandas.DataFrame: Raw data extracted from the text file
        
    Raises:
        ValueError: If the file is not a .txt file or if an error occurs during reading
    """
    ## Ensure the file has a .txt extension
    if os.path.splitext(file_path)[1].lower() != '.txt':
        raise ValueError("Unsupported file format. This function only processes .txt files.")

    try:
        ## Read the file as fixed-width format without header
        df = pd.read_fwf(file_path, encoding= _encoding, header= None)
        return df
    except Exception as e:
        raise ValueError(f"Error reading fixed-width file: {str(e)}")


def extract_dataframe_from_excel(file_path: str) -> pd.DataFrame:
    """
    Extract data from an Excel file into a pandas DataFrame.
    
    Parameters:
        file_path: Path to the input Excel file (.xls or .xlsx)
        
    Returns:
        pandas.DataFrame: Data extracted from the Excel file
        
    Raises:
        ValueError: If the file is not an Excel file or if an error occurs during reading
    """
    ## Ensure the file has an Excel extension
    ext = os.path.splitext(file_path)[1].lower()
    if ext not in ['.xls', '.xlsx']:
        raise ValueError("Unsupported file format. This function only processes Excel files.")

    try:
        ## Read the Excel file
        df = pd.read_excel(file_path)
        return df
    except Exception as e:
        raise ValueError(f"Error reading Excel file: {str(e)}")


def save_dataframe_to_excel(df: pd.DataFrame, output_path: str) -> None:
    """
    Save a DataFrame to an Excel file without overwriting an existing file.
    
    If an Excel file already exists at the provided output_path, a new file name
    will be generated by appending an underscore and a counter to the base file name.
    
    Parameters:
        df: DataFrame to save
        output_path: Full path where the Excel file will be saved
        
    Returns:
        None
    """
    ## Ensure the directory exists
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    ## Split the file name into base and extension
    base, ext = os.path.splitext(output_path)
    counter = 1
    
    ## Loop until we find a file name that doesn't exist
    while os.path.exists(output_path):
        output_path = f"{base}_{counter}{ext}"
        counter += 1
    
    ## Write the DataFrame to the new Excel file
    try:
        with pd.ExcelWriter(output_path, engine='openpyxl', mode='w') as writer:
            df.to_excel(writer, index=False)
        saved_filename = os.path.basename(output_path)
        target_directory = os.path.dirname(output_path)
        print(f"Successfully saved {saved_filename} to {target_directory}")
    except Exception as e:
        raise ValueError(f"Error writing Excel file: {str(e)}")


def parse_FC_data_1(df: pd.DataFrame) -> pd.DataFrame:
    """
    Remove columns from a DataFrame where all values are either 0, '0', or null.
    
    This function is intended for cleaning the dataframes extracted from text files
    where some columns contain only zeros or null values and don't contribute 
    meaningful information.
    
    Parameters:
        df: Input DataFrame to clean.
        
    Returns:
        pandas.DataFrame: DataFrame with zero-only columns removed.
    """
    ## Create a copy to avoid modifying the original DataFrame
    cleaned_df = df.copy()
    
    ## List to store columns that should be kept
    columns_to_keep = []
    
    for col in cleaned_df.columns:
        ## Check if column contains only 0s (numeric), '0' (string), or null values
        is_all_zeros = cleaned_df[col].apply(
            lambda x: x == 0 or x == '0' or pd.isna(x)
        ).all()
        
        ## If not all zeros, keep the column
        if not is_all_zeros:
            columns_to_keep.append(col)
    
    ## Return DataFrame with only the columns to keep
    return cleaned_df[columns_to_keep]


def parse_FC_data_2(df: pd.DataFrame) -> pd.DataFrame:
    """
    Process a DataFrame containing municipal or parish data where the first column contains 
    combined numeric + text data (e.g. "123456Municipality Name").
    
    This function splits the first column into two parts:
      - A new leftmost column containing the numeric code.
      - The original first column is replaced with only the non-numeric (text) part.
    
    Parameters:
        df: Input DataFrame.
        
    Returns:
        pandas.DataFrame: Modified DataFrame with split first column.
    """
    ## Create a copy to avoid modifying the original DataFrame
    new_df = df.copy()
    
    ## Ensure values are treated as strings
    col0 = new_df[0].astype(str)

    ## Extract numeric part from the first column using a regex
    numeric_part = col0.str.extract(r'^(\d+)')[0].astype('int64')
    
    ## Extract text part from the first column (trim any whitespace)
    text_part = col0.str.extract(r'^\d+(.*)$')[0].str.strip()
    
    ## Create a new DataFrame with the extracted parts and all remaining columns
    result_df = pd.concat([numeric_part, text_part, new_df.iloc[:, 1:]], axis=1)
    
    ## Reset the column indeces
    result_df.columns = range(len(result_df.columns))
    
    return result_df


def parse_FC76_data(df):
    """
    Process the intermediate column (column index 2) in 76 files.
    
    For 76 files, the intermediate column is a string with a fixed pattern:
      - If there's no voting data, it appears as "500{spaces}0".
      - Otherwise, it starts with extraneous zeros, contains the important numeric value, and ends with a trailing '0'.
    
    This function extracts the numeric part and returns the column as an int64.
    """
    import pandas as pd
    cleaned_df = df.copy()
    
    def process_value(val):
        # Convert value to string and strip surrounding whitespace.
        s = str(val).strip()
        # If no voting data, pattern starts with "500"
        if s.startswith("500"):
            return 5000
        else:
            # Remove trailing '0' if present.
            if s.endswith("0"):
                s = s[:-1]
            # Remove leading zeros.
            s = s.lstrip("0")
            try:
                return int(s) if s != "" else 0
            except Exception:
                return pd.NA

    cleaned_df[2] = cleaned_df[2].apply(process_value).astype("Int64")
    return cleaned_df


def create_column_mapping(_ext: str, _name: str, _df: pd.DataFrame) -> dict:
    """
    Create a dictionary mapping the DataFrame's column indices to new names based on file metadata.
    
    Parameters:
        _ext: The file extension. A mapping is only created if this is ".txt".
        _name: The base name of the file which determines naming for the first two columns.
        _df: The DataFrame to be processed.

    Returns:
        A dictionary where the keys are the original column indices and the values are the new column names.

    Naming rules:
        - Only applies when file_ext is ".txt".
        - If base_name ends with "c": first two columns correspond to municipality code and name.
        - If base_name ends with "f": first two columns correspond to parish code and name.
        - In all cases:
            Column 2 is named "num".
            Column 3-6 are total vote data.
        - For any remaining columns (from index 7 onward), assign (party acronym, vote count) pairs.
    """
    ## Only create a mapping for .txt files.
    if _ext.lower() != ".txt":
        return {}

    mapping = {}
    ## Determine names for the first two columns based on base_name.
    if _name.endswith("c"):
        mapping[0] = "municipality_code"
        mapping[1] = "municipality_name"
    elif _name.endswith("f"):
        mapping[0] = "parish_code"
        mapping[1] = "parish_name"
    else:
        ## Fallback names if base_name doesn't end with 'c' or 'f'
        mapping[0] = "code"
        mapping[1] = "name"

    ## Set fixed names for the next five columns.
    mapping[2] = "num"
    mapping[3] = "total_registered"
    mapping[4] = "total_votes"
    mapping[5] = "blank_votes"
    mapping[6] = "null_votes"

    ## For remaining columns starting at index 7, in pairs.
    ## Use the first non-null entry of each party acronym column as the column name.
    cols= _df.shape[1]
    for col in range(7, cols, 2):
        ## Extract the party acronym from the first non-null entry.
        try:
            party_acronym = _df.iloc[:, col].dropna().iloc[0]
        except IndexError:
            ## If the column is empty, use a generic name.
            party_acronym = f"party{col}"
        mapping[col] = party_acronym
        if col + 1 < cols:
            mapping[col + 1] = f"{party_acronym}_votes"
        
    return mapping


def generate_ddl_from_file(file_path: str) -> tuple[pd.DataFrame, str]:
    """
    Generate SQL Data Definition Language (DDL) statement and insertion commands from a CSV or Excel file.

    This function reads the provided file (CSV, XLS, or XLSX) into a pandas DataFrame, maps its data types to SQL
    types, and constructs a CREATE TABLE statement along with INSERT commands for each row of data. This facilitates
    an ETL process by generating both the table definition and the corresponding data insertion queries.

    Parameters:
        file_path (str): Path to the input file (.csv, .xls, or .xlsx).

    Returns:
        tuple: A tuple containing:
            - pandas.DataFrame: The data read from the input file.
            - str: The SQL DDL statement including the table creation and insertion commands.

    Raises:
        ValueError: If the file format is unsupported or if an error occurs during file reading.
    """
    ## Extract the file extension and convert it to lowercase (remove the leading dot)
    _, ext = os.path.splitext(file_path)
    file_extension = ext.lower()[1:]

    ## Read the file into a DataFrame based on its extension
    if file_extension == 'csv':
        df = pd.read_csv(file_path)
    elif file_extension in ['xls', 'xlsx']:
        df = pd.read_excel(file_path)
    else:
        raise ValueError("Unsupported file format. Please use .csv, .xls, or .xlsx files")

    ## Define mapping from pandas data types to SQL data types
    dtype_mapping = {
        'object': 'TEXT',
        'int64': 'INTEGER',
        'float64': 'NUMERIC',
        'datetime64[ns]': 'TIMESTAMP',
        'bool': 'BOOLEAN'
    }
    
    ## Extract the table name from the file path (use the file name without extension, in lowercase)
    table_name = os.path.splitext(os.path.basename(file_path))[0].lower()
    
    ## Build a list of column definitions for the CREATE TABLE statement
    columns = []
    for column, dtype in df.dtypes.items():
        sql_type = dtype_mapping.get(str(dtype), 'TEXT')
        columns.append(f'    "{column}" {sql_type}')
    
    ## Construct the CREATE TABLE DDL statement
    ddl = f"CREATE TABLE IF NOT EXISTS {table_name} (\n"
    ddl += ",\n".join(columns)
    ddl += "\n);"
    
    ## Prepare a comma-separated list of column names for the INSERT statements
    columns_list = ", ".join([f'"{col}"' for col in df.columns])
    insert_statements = []
    
    ## Generate an INSERT statement for each row in the DataFrame
    for index, row in df.iterrows():
        values = []
        for col in df.columns:
            if pd.isna(row[col]):
                values.append('NULL')
            elif isinstance(row[col], (int, float)):
                values.append(str(row[col]))
            elif isinstance(row[col], bool):
                values.append('TRUE' if row[col] else 'FALSE')
            elif isinstance(row[col], str):
                ## Escape single quotes in string values by replacing them with two single quotes
                values.append(f"'{row[col].replace('\'', '\'\'')}'")
            else:
                values.append(str(row[col]))
        values_str = ", ".join(values)
        insert_statements.append(f"INSERT INTO {table_name} ({columns_list}) VALUES ({values_str});")
    
    ## Append the INSERT statements to the DDL with a separating newline
    ddl += "\n\n" + "\n".join(insert_statements)
    ddl += "\n"
    
    return df, ddl
    