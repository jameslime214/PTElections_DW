{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main_DDL\n",
    "---\n",
    "## This file is responsible for reading the raw data, cleaning it, and transforming the data into SQL files for implementation in the server containing the database\n",
    "\n",
    "### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import openpyxl\n",
    "from myMethods import iter_file_paths, generate_ddl_from_file, process_txt_file, parse_FC_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Directory containing the data files\n",
    "DATA_dir = './Data_Selected/'\n",
    "output_dir = DATA_dir\n",
    "\n",
    "## Loop through every file in the data directory\n",
    "for filepath in iter_file_paths(DATA_dir):\n",
    "    \n",
    "    # file_path = os.path.join(DATA_dir, filepath)\n",
    "    out_name = filepath.lower().replace('.txt', '.xlsx')\n",
    "\n",
    "    ## process the text file and generate a DataFrame\n",
    "    try:\n",
    "        df = process_txt_file(filepath, _encoding= 'cp1252')\n",
    "        filename = os.path.basename(filepath)\n",
    "        print(f\"Successfully processed: {filename}\")\n",
    "        ## Parse the DataFrame to clean up the data\n",
    "        df = parse_FC_data(df)\n",
    "        print(f\"Successfully cleaned: {filename}\")\n",
    "    ## If the file is not a valid text file, skip it\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {os.path.basename(filepath)}: {e}\")\n",
    "    ## Export the DataFrame to an Excel file\n",
    "    with pd.ExcelWriter(out_name, engine='openpyxl', mode='w') as writer: df.to_excel(writer, header=False, index=False)\n",
    "    print(f\"Successfully exported: {filename} to {out_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating SQL DDL/DML scripts to transform the raw excel data into SQL tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the data files\n",
    "data_dir = './Data_Selected'\n",
    "output_dir = './sql_scripts/ddl-dml/'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Loop through every file in the data directory using the iter_file_paths function\n",
    "for file_path in iter_file_paths(data_dir):\n",
    "    file_name = os.path.basename(file_path)\n",
    "    \n",
    "    # Process the file using your SQL DDL function\n",
    "    try:\n",
    "        _, sql_script = generate_ddl_from_file(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_name}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Build the output file name based on the original file's name\n",
    "    base_name, _ = os.path.splitext(file_name)\n",
    "    output_file = os.path.join(output_dir, base_name + '.sql')\n",
    "    \n",
    "    # Write the generated SQL script to the output file\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(sql_script)\n",
    "    \n",
    "    print(f\"SQL script for {file_name} saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cells below are explicitly for testing / debugging purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            0     1   2   3     4     5   6    7    8   \\\n",
      "0                 010101AGADAO     0   0   0   474   429   1    5   AD   \n",
      "1        010102AGUADA DE BAIXO     0   0   0  1145  1024   4   13   AD   \n",
      "2         010103AGUADA DE CIMA     0   0   0  2364  2051  11   52   AD   \n",
      "3                 010104AGUEDA     0   0   0  8189  6954  44  128   AD   \n",
      "4                  010105BARRO     0   0   0  1083   904   6   19   AD   \n",
      "...                        ...   ...  ..  ..   ...   ...  ..  ...  ...   \n",
      "4219  221005SAO ROQUE DO FAIAL     0   0   0   637   582   2    3  NaN   \n",
      "4220                221006ILHA  5000   0   0     0     0   0    0  NaN   \n",
      "4221         221101BOA VENTURA     0   0   0  1361  1114   1   18  NaN   \n",
      "4222       221102PONTA DELGADA     0   0   0   988   813   1   35  NaN   \n",
      "4223         221103SAO VICENTE     0   0   0  2597  2210   7   68  NaN   \n",
      "\n",
      "          9   ...   22   23    24   25   26    27   28    29    30   31  \n",
      "0      343.0  ...  0.0  PSR   4.0  0.0  UDP   2.0  0.0  UEDS   4.0  0.0  \n",
      "1      625.0  ...  0.0  PSR   7.0  0.0  UDP  12.0  0.0  UEDS   5.0  0.0  \n",
      "2     1247.0  ...  0.0  PSR  15.0  0.0  UDP  17.0  0.0  UEDS  14.0  0.0  \n",
      "3     3206.0  ...  0.0  PSR  24.0  0.0  UDP  79.0  0.0  UEDS  35.0  0.0  \n",
      "4      467.0  ...  0.0  PSR   5.0  0.0  UDP   9.0  0.0  UEDS   8.0  0.0  \n",
      "...      ...  ...  ...  ...   ...  ...  ...   ...  ...   ...   ...  ...  \n",
      "4219     NaN  ...  0.0  PSR   1.0  0.0  UDP   6.0  0.0   NaN   NaN  NaN  \n",
      "4220     NaN  ...  NaN  NaN   NaN  NaN  NaN   NaN  NaN   NaN   NaN  NaN  \n",
      "4221     NaN  ...  0.0  PSR   6.0  0.0  UDP   5.0  0.0   NaN   NaN  NaN  \n",
      "4222     NaN  ...  0.0  PSR   2.0  0.0  UDP  39.0  0.0   NaN   NaN  NaN  \n",
      "4223     NaN  ...  0.0  PSR  13.0  0.0  UDP  17.0  0.0   NaN   NaN  NaN  \n",
      "\n",
      "[4224 rows x 32 columns]\n",
      "Successfully processed: ar79f.txt\n"
     ]
    }
   ],
   "source": [
    "## Directory containing the data files\n",
    "DATA_dir = './Data_Selected/'\n",
    "output_dir= '../testing_directory/'\n",
    "file_path = os.path.join(DATA_dir, 'AR79F.txt')\n",
    "\n",
    "## testing the process_txt_file function\n",
    "try:\n",
    "    df = process_txt_file(file_path, output_dir, _encoding= 'cp1252', _write=True)\n",
    "    filename = os.path.basename(file_path)\n",
    "    print(f\"Successfully processed: {filename}\")\n",
    "## If the file is not a valid text file, skip it\n",
    "except Exception as e:\n",
<<<<<<< HEAD
    "    print(f\"Error processing {os.path.basename(file_path)}: {e}\")\n",
    "## Parse the DataFrame to clean up the data\n",
    "df = parse_FC_data(df)\n",
    "## Export the DataFrame to an Excel file\n",
    "df.to_excel(out_name, header=False, index=False)\n"
=======
    "    print(f\"Error processing {os.path.basename(file_path)}: {e}\")"
>>>>>>> parent of 11c1648 (DATA CLEANING PREPARED)
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
